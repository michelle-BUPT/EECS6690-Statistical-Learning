---
title: "SL HW1"
author: "Tengyu Zhou, tz2338"
date: "2018/1/31"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Problem 3

```{r P3.a}
x <- rnorm(100, 0, 1)
eps <- rnorm(100, 0, 0.25)
y <- -1 + 0.5 * x + eps
```
The length of vector y is 100, $\beta_0 = -1, \beta_1 = 0.5$
```{r P3.d}
plot(x, y)
fit1 <- lm(y ~ x)
```
The scatterplot indicates there is a linear correlation hidden between X and Y, However, there also exists some noise.
$\hat\beta_0$ and $\hat\beta_1$ is almost exactly the same as $\beta_0$ and $\beta_1$ except minus difference. The model can delineate the origin model quite accurately.


```{r P3.f}
y2 <- -1 + 0.5 * x
plot(x, y, main="Scatterplot", 
  	xlab="x ", ylab="y ", pch=19)
abline(lm(y ~ x), col="red") # regression line (y~x) 
lines(x, y2, col="blue") # lowess line (x,y)
legend("bottomleft", 
  legend = c("Least Square Line", "population regression line"), 
  col = c("red", 
  "blue"), 
  pch = c(17,19), 
  bty = "n", 
  pt.cex = 2, 
  cex = 1.2, 
  text.col = "black", 
  horiz = F , 
  inset = c(0.3, 0.1))
```
```{r P3.g}
model <- lm(y ~ poly(x,2))
```
Adjusted R-squared:  0.8351 for linear model
Adjusted R-squared:  0.8364 for polynomial model
There is no evidence that polynomial model improves the model fit. Since the original model is linear, so using extra parameters actually doesn't help at all. We can see the R-squared for both is approximately the same.

```{r P3.h}
x <- rnorm(100, 0, 1)
eps <- rnorm(100, 0, 0.05)
y <- -1 + 0.5 * x + eps
y2 <- -1 + 0.5 * x
plot(x, y)
fit1 <- lm(y ~ x)
plot(x, y, main="Scatterplot", 
  	xlab="x ", ylab="y ", pch=19)
abline(lm(y ~ x), col="red") # regression line (y~x) 
lines(x,y2, col="blue") 
legend("bottomleft", 
  legend = c("least square line", "population regression line"), 
  col = c("red", 
  "blue"), 
  pch = c(17,19), 
  bty = "n", 
  pt.cex = 2, 
  cex = 1.2, 
  text.col = "black", 
  horiz = F , 
  inset = c(0.3, 0.1))
```
The result of decreasing noise is the linear regression model more closely allign to the original model.

```{r P3.i}
x <- rnorm(100, 0, 1)
eps <- rnorm(100, 0, 0.65)
y <- -1 + 0.5 * x + eps
y2 <- -1 + 0.5 * x
plot(x, y)
fit2 <- lm(y ~ x)
plot(x, y, main="Scatterplot", 
  	xlab="x ", ylab="y ", pch=19)
abline(lm(y ~ x), col="red") # regression line (y~x) 
lines(x,y2, col="blue") # lowess line (x,y)
legend("bottomleft", 
  legend = c("least square line", "population regression line"), 
  col = c("red", 
  "blue"), 
  pch = c(17,19), 
  bty = "n", 
  pt.cex = 2, 
  cex = 1.2, 
  text.col = "black", 
  horiz = F , 
  inset = c(0.3, 0.1))
```

After adding more noise to the model, the linear regression cannot accurately predict the model because of the noise. But the overall result or the RSS is still quite low probably because we are using a linear model to predict a linear model.

```{r P3.j}
confint(model)
confint(fit1)
confint(fit2)
```
(j)The confidence interval for $\beta_0$ and $\beta_1$ is
-1.000134 -0.9075795, 4.784002  5.7095476 for original dataset,
-1.0824224 -0.8303113, 0.2459571  0.5668774 for noisy dataset and
-1.089278 -0.8500929, 0.509077  0.7592205 for less noisy dataset respectly.

##P4

```{r P4}
w1 <- read.csv(file="advertising.csv",head=TRUE)
sale <- w1$sales
tv <- w1$TV
fitTV <- lm(sale ~ tv)
confint(fitTV, level=0.92)
plot(tv, sale, xlab = "TV", ylab = "Sales", col = "red", pch = 15,
xlim=c(0,150),ylim=c(0,30))
abline(fitTV, col = "blue", lwd = 2)
newx<-seq(0,150)
prd1<-predict(fitTV,newdata=data.frame(tv=newx),interval =c("confidence"),level = 0.92, type="response")
lines(newx,prd1[,2],col="blue",lty=2)
lines(newx,prd1[,3],col="blue",lty=2)
legend(80, 20, legend=c("The original data", "Linear Regression", "Confidnece Interval"),
col=c("red", "blue", "blue"),lty=c(1,1,2), cex=0.6)
```

                   4 %       96 %
(Intercept) 6.22691926 7.83826784
tv          0.04280193 0.05227135

```{r P4.1}
w1 <- read.csv(file="advertising.csv",head=TRUE)
sale <- w1$sales
radio <- w1$radio
fitradio <- lm(sale ~ radio)
confint(fitradio, level=0.92)
plot(radio, sale, xlab = "radio", ylab = "Sales", col = "red", pch = 15,
xlim=c(0,70),ylim=c(0,30))
abline(fitradio, col = "blue", lwd = 2)
newx<-seq(0,70)
prd1<-predict(fitradio,newdata=data.frame(radio=newx),interval =c("confidence"),level = 0.92, type="response")
lines(newx,prd1[,2],col="blue",lty=2)
lines(newx,prd1[,3],col="blue",lty=2)
legend(80, 20, legend=c("The original data", "Linear Regression", "Confidnece Interval"),
col=c("red", "blue", "blue"),lty=c(1,1,2), cex=0.6)
```
                  4 %       96 %
(Intercept) 8.3210922 10.3021840
radio       0.1665776  0.2384139

```{r P4.2}
w1 <- read.csv(file="advertising.csv",head=TRUE)
sale <- w1$sales
newspaper <- w1$newspaper
fitnewspaper <- lm(sale ~ newspaper)
confint(fitnewspaper, level=0.92)
plot(newspaper, sale, xlab = "newspaper", ylab = "Sales", col = "red", pch = 15,
xlim=c(0,150),ylim=c(0,30))
abline(fitnewspaper, col = "blue", lwd = 2)
newx<-seq(0,150)
prd1<-predict(fitnewspaper,newdata=data.frame(newspaper=newx),interval =c("confidence"),level = 0.92, type="response")
lines(newx,prd1[,2],col="blue",lty=2)
lines(newx,prd1[,3],col="blue",lty=2)
legend(80, 20, legend=c("The original data", "Linear Regression", "Confidnece Interval"),
col=c("red", "blue", "blue"),lty=c(1,1,2), cex=0.6)
```
                    4 %        96 %
(Intercept) 11.25788302 13.44493112
newspaper    0.02552451  0.08386169


##P5
```{r P5}
w2 <- read.csv(file="auto.csv",head=TRUE)
pairs(~mpg+cylinders+displacement+weight+horsepower+acceleration+year+origin+name,data=w2, 
   main="Scatterplot Matrix")
my_data <- w2[, c(1,2,3,5,6,7,8)]
res <- cor(my_data)
P5model <- lm(w2$mpg ~ w2$cylinders+w2$displacement+w2$weight+w2$horsepower+w2$acceleration+w2$year+w2$origin)
summary(P5model)
```

(b) The matrix of correlatoin is below.
                    mpg  cylinders displacement     weight
mpg           1.0000000 -0.7762599   -0.8044430 -0.8317389
cylinders    -0.7762599  1.0000000    0.9509199  0.8970169
displacement -0.8044430  0.9509199    1.0000000  0.9331044
weight       -0.8317389  0.8970169    0.9331044  1.0000000
acceleration  0.4222974 -0.5040606   -0.5441618 -0.4195023
year          0.5814695 -0.3467172   -0.3698041 -0.3079004
origin        0.5636979 -0.5649716   -0.6106643 -0.5812652
             acceleration       year     origin
mpg             0.4222974  0.5814695  0.5636979
cylinders      -0.5040606 -0.3467172 -0.5649716
displacement   -0.5441618 -0.3698041 -0.6106643
weight         -0.4195023 -0.3079004 -0.5812652
acceleration    1.0000000  0.2829009  0.2100836
year            0.2829009  1.0000000  0.1843141
origin          0.2100836  0.1843141  1.0000000

(c)

i.Some predictors have relationships with response while some appear not. For example, the cylingders predictor seems to be totally unrelated to the response. 
ii.The weight, year and origin seem to have a really strong relationship to the response since their coefficients are really large.
iii The year suggests that the old car tends to have larger mpg and there are positively related.

```{r p5.2}
P5.2model <- lm(log(w2$mpg) ~ log(w2$cylinders)+log(w2$displacement)+log(w2$weight)+log(w2$horsepower)+log(w2$acceleration)+log(w2$year)+log(w2$origin))
summary(P5.2model)
```
```{r p5.3}
P5.3model <- lm(sqrt(w2$mpg) ~ sqrt(w2$cylinders)+sqrt(w2$displacement)+sqrt(w2$weight)+sqrt(w2$horsepower)+sqrt(w2$acceleration)+sqrt(w2$year)+sqrt(w2$origin))
summary(P5.3model)
```

```{r p5.4}
P5.4model <- lm((w2$mpg) ~ (w2$cylinders^2)+(w2$displacement^2)+(w2$weight^2)+(w2$horsepower^2)+(w2$acceleration^2)+(w2$year^2)+(w2$origin^2))
summary(P5.4model)
```


The findings are that even using the same variable, but after we perform some operations on the variables. The correlation results we get will be significantly different from others. So we have to try different pre-proccesing methods to get the best results.