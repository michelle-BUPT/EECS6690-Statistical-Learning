---
title: "Lt2667_HW3"
author: "Linlin Tian"
date: "10/13/2018"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Problem 3
```{r p3}
library(matlib)
x <- c(0, 0.2, 0.4, 0.6, 0.8, 1.0)
y <- c(0, 0, 0, 1, 0, 1)

g <- function(beta){
  f1 <- sum(((exp(beta[1] + beta[2] * x)) / (1 + exp(beta[1] + beta[2] * x))) - y)
  f2 <- sum((((exp(beta[1] + beta[2] * x)) / (1 + exp(beta[1] + beta[2] * x))) - y) * x)
  matrix(c(f1, f2), ncol = 1, nrow = 2)
}

J <- function(beta){
  f11<-sum((exp(beta[1]+beta[2]*x))/(1+exp(beta[1]+beta[2]*x))^2)
  f12<-sum(x*exp(beta[1]+beta[2]*x)/((1+exp(beta[1]+beta[2]*x))^2))
  f21<-f12
  f22<-sum((x^2)*exp(beta[1]+beta[2]*x)/((1+exp(beta[1]+beta[2]*x))^2))
  matrix(c(f11,f21,f12,f22),ncol = 2,nrow = 2)
}

beta<-c(-1,1)
#beta <- c(-1, 1)
cat("Newton's method - starting point : beta0 =",beta[1],", beta1= ",beta[2])
for(i in 1 : 10){
  beta <- beta - inv(J(beta))%*%g(beta)
  cat('\n',"Newton's method- iteration" ,i, "beta0=",beta[1],", beta1=",beta[2])
}
model_glm<-glm(y~x,family = "binomial")
cat("\nBy using logistic regression function, beta0=",model_glm$coefficients[1], ", beta1=",model_glm$coefficients[2])
```

## Problem 6
```{r p6}
prob <- c(0.1, 0.15, 0.2, 0.2, 0.55, 0.6, 0.6, 0.65, 0.7, 0.75)
a <- sum(prob >= 0.5)
b <- sum(prob < 0.5)
a > b
mean(prob)
```

We can see that when using majorty vote, the prediction of red is more then that of green, then the result is red. However, in the average approach, the average probability is less then 0.5, then the result is green.

## Problem 7
```{r p7a}
set.seed(1000)
require(ISLR)
part <- sample(1:nrow(OJ), 800)
trainset <- OJ[part, ]
testset <- OJ[-part, ]
```

```{r p7b}
library(tree)
tree_OJ <- tree(Purchase ~ ., data = trainset)
summary(tree_OJ)
```

There are 7 terminal nodes in the tree and the traning error is 0.175. There are 3 variables in total which are loyalCH, PriceDiff, WeekofPurchase.

```{r p7c}
tree_OJ
```
I pick the terminal node of 7 and the way that the tree split is whether LoyalCH is larger than 0.753545. There are 268 values in this region in total and the deviance is 104.000. The prediction at this node is CH.

```{r p7d}
plot(tree_OJ)
text(tree_OJ)
```
LoyalCH is the most important variable of the tree, in fact top 3 nodes contain LoyalCH. If LoyalCH < 0:035, the tree predicts MM. If LoyalCH > 0:7535, the tree predicts CH. 

```{r p7e}
pred_OJ <- predict(tree_OJ, testset, type = "class")
table(testset$Purchase, pred_OJ)
```

Test error is 19.6%.

```{r p7f}
cv_OJ <- cv.tree(tree_OJ, FUN = prune.tree)
```


```{r p7g}
plot(cv_OJ$size, cv_OJ$dev, type = "b", xlab = "Tree Size", ylab = "Deviance")
```

```{r p7h}
which.min(cv_OJ$size)
```
Size 7 corresponds to the lowest cross-validated classification error rate.


```{r p7i}
pruned_OJ <- prune.tree(tree_OJ, best = 7)
```

```{r p7j}
summary(pruned_OJ)
```
The error is the same as the original tree.

```{r p7k}
pred_ori <- predict(tree_OJ, testset, type = "class")
error_ori <- sum(testset$Purchase != pred_ori)
error_ori/length(pred_ori)

pred_pruned <- predict(pruned_OJ, testset, type = "class")
error_pruned <- sum(testset$Purchase != pred_pruned)
error_pruned/length(pred_pruned)

```

The error of both tree is the same.
